\subsection{Model Evaluation,Validation and Justification}
%The final model’s qualities — such as parameters — are evaluated in detail. Some type of analysis is used to validate the robustness of the model’s solution.

By utilizing the bayesian optimization, I optimized the number of dimensions of first layer of fully connected layer and learning rate. I got the optimal number of first layer of fully connected layer and the learning rate of the optimizer.Those numbers are 204 and 0.000597,respectively. By using those numbers, I got the test accuracy which is 72.1\%


From the result of Bayesian Optimization, I chose 204 of first fully connected layer and 0.000597 of learning rate as the final model parameters.


The training and test accuracy in this model is as follows(Fig.13).


\begin{figure}[H]

	\begin{center}
	\includegraphics[width=7cm]{picture/final_model_accuracy.png}
	\caption{Accuracy rate of training and validation data}
	\end{center}
	\label{fig:13}

\end{figure}


The loss of the model is shown in Fig.14

 \begin{figure}[H]

	\begin{center}
	\includegraphics[width=7cm]{picture/final_loss.png}
	\caption{Loss of training and validation data}
	\end{center}
	\label{fig:14}

\end{figure}



%The final results are compared to the benchmark result or threshold with some type of statistical analysis. Justification is made as to whether the final model and solution is significant enough to have adequately solved the problem.

The model improves the accuracy rate by 2.0\%. The final model doesn't seem to fall into the overfitting according to the Fig.13 and when compared to the Fig.11 which is the benchmark result, the difference between the training and test accuracy rate is much smaller. That means the final model is more robust than the benchmark model. Therefore, the model improved compared to the benchmark result.

I only optimize two parameters, learning rate and the number of first fully connected layer's neurons. The number of random sampling points of finding the optimal score in bayesian network is 25.  When using grid search, we can only search 5 values for each parameters(5$\times5$=25). Apparently 5 different values for each parameter is not enough to search optimal parameters for grid search. Therefore, much more different values are necessary, resulting much computationally expensive. The difference will be much distinguishable when I try to optimize more parameters. In this case, grid search won't be a choice to find optimal parameter because of the scalability.  As for bayesinan optimization, it is highly possible that I can't find the best parameters within 25 sample points. However, the parameters found within 25 sample points are better result than the benchmark model parameters. When I choose more sample points, it will probably get better results. 

%The training and validation loss in the benchmark is Fig---. The counterpart in final result is Fig.---


%The number of miss-classified data in benchmark is ----. On the other hand, the number in the final result is ----.The distribution difference in miss-classified data between the benchmark and the final result is following.
