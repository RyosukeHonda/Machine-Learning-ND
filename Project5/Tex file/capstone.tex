\documentclass[a4paper,10pt,fleqn]{article}
\usepackage[dvipdfmx]{graphicx}
\title{{\Huge Capstone Project}}%%% \\Machine Learning Engineer Nanodegree}

\author{Ryosuke Honda}
\date{\today}
\begin{document}
\maketitle

\section{Definition}

\input{Tex_Section1/Project_Overview}
\input{Tex_Section1/Problem_Statement}
\input{Tex_Section1/Metrics}


\section{Analysis}
\subsection{Data Exploration}
If a dataset is present, features and calculated statistics relevant to the problem have been reported and discussed, along with a sampling of the data. In lieu of a dataset, a thorough description of the input space or input data has been made. Abnormalities or characteristics about the data or input that need to be addressed have been identified.


CIFAR-10 dataset consists of 50,000 images of training data and 10,000 images of test data.For each data, it contains 10 different kind of classes(discussed in the next chapter) and the distribution of each class is the same in both training and test data. That means 5,000 images for each class in the training data and 1,000 images for each class in the test data.(Figure.\ref{fig:three} and Figure.4). 


\subsection{Exploratory Visualization}
A visualization has been provided that summarizes or extracts a relevant characteristic or feature about the dataset or input data with thorough discussion. Visual cues are clearly defined.


There are 60,000 of images in total. Figure(50,000 images for training data and 10,000 images for test data).\ref{fig:two} shows the samples of the images. I plotted 10 images for each class.Figure.\ref{fig:three} and Figure.\ref{fig:four} shows the distribution of the data. For the training dataset, each class has 5,000 images and for the test dataset, each class has 1,000 images.
The label 0 to 9 corresponds to 'Airplane','Automobile','Bird','Cat','Deer','Dog','Frog','Horse','Ship, and 'Truck'.
\begin{figure}[htbp]

\begin{center}
\includegraphics[width=10cm]{picture/random_sample.png}
\end{center}
\caption{Sample of the Images}
\label{fig:two}

\end{figure}

\begin{figure}[h]
\begin{minipage}{0.5\hsize}
	\begin{center}
	\includegraphics[width=5cm]{picture/Distribution_of_Training_Data.png}
	\end{center}
	\caption{Distribution of Training Data}
	\label{fig:three}
\end{minipage}
\begin{minipage}{0.5\hsize}
\begin{center}
\includegraphics[width=5cm]{picture/Distribution_of_Test_Data.png}
\end{center}
 \caption{Distribution of Test Data}
  \label{fig:four}
 \end{minipage}
\end{figure}


\subsection{Algorithms and Techniques}
Algorithms and techniques used in the project are thoroughly discussed and properly justified based on the characteristics of the problem.

In this task, I'll use Convolutional Neural Network(CNN). CNN has been successful in  practical applications for image recognition.CNN consists of convolution layer, pooling layer and fully-connected layer and sometimes contains local contrast normalization(LCN). In this chapter, I'll discuss the convolution layer and pooling.
As the name 'convolution' suggests, the network employs a mathematical operation called convolution. 

Figure.5 is the example of the architecture of the CNN.
\begin{figure}[htbp]

	\begin{center}
	\includegraphics[width=10cm]{picture/Architecture_of_CNN.png}
	\caption{An example of CNN Architecture}
	\end{center}
	\label{fig:five}

\end{figure}




\begin{figure}[htbp]

	\begin{center}
	\includegraphics[width=10cm]{picture/Structure_of_convolution.png}
	\caption{Example of Max Pooling and Average Pooling}
	\end{center}
	\label{fig:six}

\end{figure}











Pooling layer is put after the convolution layer. %Pooling layer reduces the dimensionality of the input image. 
The function of the pooling layer is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting.
By introducing pooling layer, not only the architecture will be more robust but also the dimensionality will be reduced. Max pooling and Average pooling are the typical pooling which are generally utilized. Figure.7 is the example of the pooling.The original map is the size of 4x4. The stride for the pooling is 2 and the pooling size is 2x2. "Max pooling" is to extract the maximum pixel from each region and "Average pooling" is to calculate the average value for each region.In this task, I utilized max pooling at the pooling layers.

\begin{figure}[htbp]

	\begin{center}
	\includegraphics[width=10cm]{picture/Pooling.png}
	\caption{Example of Max Pooling and Average Pooling}
	\end{center}
	\label{fig:seven}

\end{figure}


\subsection{Benchmark}
Student clearly defines a benchmark result or threshold for comparing performances of solutions obtained.

For the CNNs architecture, it is quite important to decide the number of layers.Therefore, I first choose several convolutional layers and decide one of them as a benchmark.

When deciding the architecture, I set the mutual parameters as follows.
The number of batch size is 32.The number of filters of each convolutional layer is 32. and finally the number of epochs is 20.

I tried 4 architecture of CNNs.

\begin{enumerate}
 \item 1 Convolutional layer  2 Fully connected layers \\
 The simplest version in these model.The input and output dimension on each layer are below.
 The accuracy rate of training and validation data are also below.
 
 \begin{figure}[htbp]

	\begin{center}
	\includegraphics[width=5cm]{picture/1layer_cnn.png}
	\caption{Accuracy rate of training and validation data}
	\end{center}
	\label{fig:eight}

\end{figure}
 
 \item 2 Convolutional layers  2 Fully connected layers \\
 The input and output dimension on each layer are below.
 The accuracy rate of training and validation data are also below.
 
 \begin{figure}[htbp]

	\begin{center}
	\includegraphics[width=5cm]{picture/2layer_cnn.png}
	\caption{Accuracy rate of training and validation data}
	\end{center}
	\label{fig:nine}

\end{figure}


 \item 3 Convolutional layers 2 Fully connected layers \\
 The accuracy rate of training and validation data are also below.
 
 \begin{figure}[htbp]

	\begin{center}
	\includegraphics[width=5cm]{picture/3layer_cnn.png}
	\caption{Accuracy rate of training and validation data}
	\end{center}
	\label{fig:ten}

\end{figure}
 \item 4 Convolutional layers 2 Fully connected layers \\
 The accuracy rate of training and validation data are also below.
 
 \begin{figure}[htbp]

	\begin{center}
	\includegraphics[width=5cm]{picture/4layer_cnn.png}
	\caption{Accuracy rate of training and validation data}
	\end{center}
	\label{fig:eleven}

\end{figure}
\end{enumerate}


From these results, I choose --------- as the benchmark for this task.

\section{Methodology}
\subsection{Data Preprocessing}
All preprocessing steps have been clearly documented. Abnormalities or characteristics about the data or input that needed to be addressed have been corrected. If no data preprocessing is necessary, it has been clearly justified.

Apart from the models of image processing or computer vision, CNN doesn't need complex preprocessing.However,when analyzing the data, data preprocessing plays a crucial role. One of the first steps is the normalization of the data. This step is essential when dealing with parameters of different units and scales. In this dataset, pixel values range from 0 to 255. I process normalization to this dataset.
Normalization scales all numeric variables in the range of [0,1].The formula is given below.

\begin{eqnarray}
x_{new}=\frac{x-x_{min}}{x_{max}-x_{min}}
\end{eqnarray}

The minimum pixel value is 0 and maximum pixel value is 255. Therefore, I normalized the data by following.



\begin{eqnarray}
x_{new}=\frac{x}{255}
\end{eqnarray}

\subsection{Implementation}
The process for which metrics, algorithms, and techniques were implemented with the given datasets or input data has been thoroughly documented. Complications that occurred during the coding process are discussed.






\subsection{Refinement}
The process of improving upon the algorithms and techniques used is clearly documented. Both the initial and final solutions are reported, along with intermediate solutions, if necessary.

Finding the optimal parameters for deep learning is quite difficult though it is important. When it comes to typical machine learning algorithm(Decision tree, Support Vector Machine etc.),grid search is taken to search the optimal parameters. However, it's almost impossible to apply grid search in deep learning because of the computational time. Therefore, other methods are indispensable. A good choice is Bayesian optimization, which has been shown to outperform other state of the art global optimization algorithms on a number of challenging optimization benchmark functions.




\section{Result}
\subsection{Model Evaluation and Validation}
The final model’s qualities — such as parameters — are evaluated in detail. Some type of analysis is used to validate the robustness of the model’s solution.

From the result of Bayesian Optimization, I chose ----- as the final model parameters.



\subsection{Justification}
The final results are compared to the benchmark result or threshold with some type of statistical analysis. Justification is made as to whether the final model and solution is significant enough to have adequately solved the problem.




\section{Conclusion}
\subsection{Free-Form Visualization}
A visualization has been provided that emphasizes an important quality about the project with thorough discussion. Visual cues are clearly defined.

\subsection{Reflection}
Student adequately summarizes the end-to-end problem solution and discusses one or two particular aspects of the project they found interesting or difficult.

\subsection{Improvement}
Discussion is made as to how one aspect of the implementation could be improved. Potential solutions resulting from these improvements are considered and compared/contrasted to the current solution.






\end{document}